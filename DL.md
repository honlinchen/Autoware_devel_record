# DL_CNN

## 深度学习基础
        语义鸿沟 Semantic Gap：
        图像的底层视觉特性和高层语义概念之间的鸿沟
        相似的视觉特性，不同的语义概念 && 不相似的视觉特性，相同的语义概念
### 图像识别基本框架
        测量空间——特征空间——类别空间
                |                        |
        特征设计/学习特征   特征匹配
    全局特征提取：例如颜色，形状，纹理——>向量空间映射——>向量表示
### CNN
```
DNN全连接网络不够灵活，参数量太大 ——>CNN：局部连接—权重共享—下采样——>减少网络参数
局部连接：人眼观察的时候也是关注局部，局部移动，
权重共享：卷积层的某一个
下采样：减小分辨率
  input image——  kernel:卷积核—需要学习的对象——feature map(w*h*channel)
  
 目标检测相对于图像分类而言多了一个画框，所以损失函数要采用回归类的
 
 【清华年青老师的梯度下降文章，引用数千，研究怎么跳过鞍部 ？】
 
 实践：
 动态图：DyGraph，更好的coding与debug卷积步长，池化步长都是调参的对象，遵守矩阵乘法律
 iter*batchsize)/sum_photos=epoch  iter指的是整个batchsize输入到网络多少次

Q：怎么设置是否为全连接层，有一个属性可以设置吗？不是全连接层那怎么控制连接方式？
      可以肯定卷积层不是全连接层，并且卷积层有自己的连接方式
      
口罩检测：
图像分类：
目标检测：
图像语义分割：
图像实例分割：

模型校验：使用验证数据？
数据增强使用的是python的接口，一般框架都不会提供
dropout:
ealy stop:提前停止训练循环，
优化函数的作用？ 

数据集：Minist ImageNet
局部特征描述子 VGG  
2015 何凯明ResNet

AlexNet提出Relu , 使用边界延拓，使用两个GPU，分组卷积48+48
Relu使用，不再有梯度消失的问题

VGG:
CONV3_64:64个3*3的卷积核

GoogleNet:
Inception: 同层里使用不同大小的卷积核，
1*1的结构可以降参数量，可以使得“通道数”能随意变化
Global Average OPooling :使用一个统计值代替卷积层的结果

轻量级网络：1*1*32，1*1*n?  将channel数先降下来再升上去，通道数就是卷积核数？

ResNet:
针对梯度不稳定的问题，当梯度不处理好时，深层网络不如浅层的
现象：网络越深，梯度越加不可控
残差网络：一种控制梯度稳定的方法



 

```

## Ai Studio（百度）

```
wor1: Paddle架构，pyecharts图表库，python爬虫（json , re, requests）
DNN：可以认为是线性网络，对于层的节点数的设置需要满足矩阵乘法规则：MN NJ = MJ
深度网络：随着深度的增加，可以达到不同程度信息特征的学习和表达；虽然深度网络类似于黑箱操作，但在设计深度网络时应当遵循可解释的/可理解的思路

```

# 概率论

- [Ref 1](https://www.cnblogs.com/ycwang16/p/5995702.html) 
- [Ref 2](https://blog.csdn.net/renhaofan/article/details/82144274)
- [Ref 3](https://blog.csdn.net/renhaofan/article/details/82415167)

## 1.基本概念

* 联合概率:
< p(X=x  and  Y=y)=p(x,y)，称为联合概率密度分布。如果X和Y是相互独立的随机变量，p(x,y)=p(x)p(y)。一般的，课本上的写法是：P(AB)或者P(A,B)或者P(A*B)，表达A， 
< B共同发生的概率。有所区别的是课本上的A，B表示的是事件，p(x,y)表示的是随机变量X=x,随机变量Y=y。两者本质一致，本文统一使用p(x,y)表示。
                       
* 相互独立：
< X,Y相互独立，则X=x下的发生与Y无关，Y=y的发生与X无关，有：p(x|y)=p(x)。相互独立，结合联合概率也可以得出：p(x,y)=p(x)p(y)。
                       
* 随机过程：
< 当两个随机变量进行对比的时候，出现了相互独立，以及相关性；当一个随机变量的不同阶段进行对比的时候，就会出现一个新概念：随机过程。随机过程 
< [Ref](https://baike.baidu.com/item/%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B/368895)
  
* 马尔科夫过程:
< 当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史
< 路径）是条件独立的，那么此随机过程即具有马尔可夫性质。具有马尔可夫性质的过程通常称之为马尔可夫过程。【百度】
                       

## 链式法则？
