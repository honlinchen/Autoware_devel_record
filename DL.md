# DL_CNN

## 深度学习基础
        语义鸿沟 Semantic Gap：
        图像的底层视觉特性和高层语义概念之间的鸿沟
        相似的视觉特性，不同的语义概念 && 不相似的视觉特性，相同的语义概念
### 图像识别基本框架
        测量空间——特征空间——类别空间
                |                        |
        特征设计/学习特征   特征匹配
    全局特征提取：例如颜色，形状，纹理——>向量空间映射——>向量表示
### CNN
```
DNN全连接网络不够灵活，参数量太大 ——>CNN：局部连接—权重共享—下采样——>减少网络参数
局部连接：人眼观察的时候也是关注局部，局部移动，
权重共享：卷积层的某一个
下采样：减小分辨率
  input image——  kernel:卷积核—需要学习的对象——feature map(w*h*channel)
  
 目标检测相对于图像分类而言多了一个画框，所以损失函数要采用回归类的
 
 【清华年青老师的梯度下降文章，引用数千，研究怎么跳过鞍部 ？】
 
 实践：
 动态图：DyGraph，更好的coding与debug卷积步长，池化步长都是调参的对象，遵守矩阵乘法律
 iter*batchsize)/sum_photos=epoch  iter指的是整个batchsize输入到网络多少次

Q：怎么设置是否为全连接层，有一个属性可以设置吗？不是全连接层那怎么控制连接方式？
      可以肯定卷积层不是全连接层，并且卷积层有自己的连接方式
      
口罩检测：
图像分类：
目标检测：
图像语义分割：
图像实例分割：

模型校验：使用验证数据？
数据增强使用的是python的接口，一般框架都不会提供
dropout:
ealy stop:提前停止训练循环，
优化函数的作用？ 

数据集：Minist ImageNet
局部特征描述子 VGG  
2015 何凯明ResNet

AlexNet提出Relu , 使用边界延拓，使用两个GPU，分组卷积48+48
Relu使用，不再有梯度消失的问题

VGG:
CONV3_64:64个3*3的卷积核

GoogleNet:
Inception: 同层里使用不同大小的卷积核，
1*1的结构可以降参数量，可以使得“通道数”能随意变化
Global Average OPooling :使用一个统计值代替卷积层的结果

轻量级网络：1*1*32，1*1*n?  将channel数先降下来再升上去，通道数就是卷积核数？

ResNet:
针对梯度不稳定的问题，当梯度不处理好时，深层网络不如浅层的
现象：网络越深，梯度越加不可控
残差网络：一种控制梯度稳定的方法



 

```

## Ai Studio（百度）

```
wor1: Paddle架构，pyecharts图表库，python爬虫（json , re, requests）
DNN：可以认为是线性网络，对于层的节点数的设置需要满足矩阵乘法规则：MN NJ = MJ
深度网络：随着深度的增加，可以达到不同程度信息特征的学习和表达；虽然深度网络类似于黑箱操作，但在设计深度网络时应当遵循可解释的/可理解的思路

```

# 概率论

## 贝叶斯

- [Ref 1](https://www.cnblogs.com/ycwang16/p/5995702.html) 贝叶斯滤波
- [Ref 2](https://blog.csdn.net/renhaofan/article/details/82144274)贝叶斯相关公式
- [Ref 3](https://blog.csdn.net/renhaofan/article/details/82415167)贝叶斯滤波

### 1.基本概念

- 随机变量（random variable）<br>
表示随机试验各种结果的实值单值函数

* 相互独立:<br>
[百度解释](https://baike.baidu.com/item/%E7%9B%B8%E4%BA%92%E7%8B%AC%E7%AB%8B/4475829?fr=aladdin):设A,B是试验E的两个事件,若P(A)>0,可以定义P(B∣A).一般A的发生对B发生的概率是有影响的,所以条件概率P(B∣A)≠P(B),而只有当A的发生对B发生的概率没有影响的时候（即A与B相互独立）才有条件概率P(B∣A)=P(B). 这时,由乘法定理P(A∩B)=P(B∣A)P(A)=P(A)P(B).   因此定义:
```
                设A,B是两事件,如果满足等式P(A∩B)=P(AB)=P(A)P(B),则称事件A,B相互独立,简称A,B独立.
                即：A，B相互独立 <=> P(AB)=P(A)P(B)
```
* 条件概率<br>
A发生后，B发生的概率：P(B∣A)。有两种情况：
```
P(B∣A)≠P(B)    A，B不相互独立
P(B∣A)≠P(B)    A，B相互独立
```

* 联合概率:<br>
 p(X=x  and  Y=y)=p(x,y)，称为联合概率密度分布。如果X和Y是相互独立的随机变量，p(x,y)=p(x)p(y)。一般的，课本上的写法是：P(AB)或者P(A,B)或者P(A * B)，表达A， 
 B共同发生的概率。有所区别的是课本上的A，B表示的是事件，p(x,y)表示的是随机变量X=x,随机变量Y=y。两者本质一致，本文统一使用p(x,y)表示。
                       
* 随机过程1：单变量随机过程<br>
当两个随机变量进行对比的时候，出现了相互独立，以及相关性；当一个随机变量的不同阶段进行对比的时候，就会出现一个新概念：随机过程。随机过程 
[Ref](https://baike.baidu.com/item/%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B/368895)百度。

* 随机过程2：多变量随机过程 <br>
未给定独立性前提的随机变量集合X={X1,X2,X3.....Xn}，通过研究相关性，协相关性等可以得到时间截面t0，X的成员的纵向关系；通过研究各个变量的随机过程，可以得到多变量集合X随时间T的系统运动过程特征。
  
* 马尔科夫过程:<br>
当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史
路径）是条件独立的，那么此随机过程即具有马尔可夫性质。具有马尔可夫性质的过程通常称之为马尔可夫过程。【百度】

- 贝叶斯公式：参看Ref2
```
P(A|B)=P(B|A)P(A)/P(B)
 A一般是某种状态，B一般是某种观测值
1.P(A)先验概率（Prior probability）
2.P(A|B)后验概率（Posterior/causal probability)
3.P(B|A)/P(B)可能性函数（Likelyhood）
```
分析：B发生并且经过观测后，A的概率可以因为B发生以及观测而得到修正，修正的方式是乘以一个可能性函数Likelyhood，即：P(A|B)=L(B) * P(A)，L(B)的推导用到了相互独立随机变量假设，条件概率公式，联合概率公式。
         

### 2. 

## 链式法则？
